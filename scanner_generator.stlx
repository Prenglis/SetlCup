/*
  In dieser Datei wird die Eingabe analysiert. Es wird der Scanner aus den angegebenen Tokens generiert. 
  Dieser wird daraufhin in einer Ausgabedatei abgelegt. 
  Auch der Parser aus der Datei "_sr_parser_part.stlx" wird der Datei angehangen. 
  Zusätzlich wird die Grammatikdefinition analysiert und  in einer Token-Liste zurückgegeben.
*/
sc_dict_generate     := procedure(file, silent_mode){
  file               := join(readFile(file), "\n");
  state              := "comment_part";
  last_identifier    := "";
  parser_part_string := "";
  sc_dict            :=  {};
  parser_token_list  := [];
  user_code          := "";
  comment := "";
  if(!silent_mode){
    print("grammar will now be scanned");
  }
  scan(file) using map{
    regex '^[^%]*' as [c] | state == "comment_part":
      comment += c;
    regex '%%%.*\n' | state == "comment_part":
      state := "scanner_part";
      // skip intro
    regex '[\n]*%%%.*' | state == "scanner_part":     
      state := "parser_part";
    regex '[ \t]*\n' | state == "scanner_part":
      //skip new lines
    regex '[A-Z]+' as [ identifier ] | state == "scanner_part":
      // save identifier
      sc_dict[identifier] := "om";
      last_identifier := identifier;
      state := "ident_read";
    regex '[\s]*:=[\s]*' | state == "ident_read":
      // skip white spaces
      state :=  "scan_section";
    regex '.*' as [ scan_section ] | state == "scan_section":
      scan_section := trim(scan_section);
      scan_section := replaceFirst(scan_section, "[\\s]*;\$", "");
      scan_section := replace(scan_section, '"', '\\\"');
      sc_dict[last_identifier] := scan_section;
      state := "scanner_part";
    regex '::=|\||;' as [o] | state == "parser_part":
      // skip parser part
      parser_token_list += [ o        ];
    regex '[a-z][a-zA-Z_0-9]*' as [ v ] | state == "parser_part": 
      var_id := ""; 
      parser_token_list += [ Var(v, var_id) ] ;
    regex '[a-z][a-zA-Z_0-9]*:[a-z][a-zA-Z_0-9]*' as [ v ] | state == "parser_part": 
      [var_name, var_id] := split(v, ":"); 
      parser_token_list += [ Var(var_name, var_id) ];
    regex '[A-Z][A-Z_0-9]*'    as [ t ] | state == "parser_part": 
        [token_name, token_id] := [t, ""];
      parser_token_list +=  [ Token(token_name, token_id) ];
    regex '[A-Z][A-Z_0-9]*:[a-z][a-zA-Z_0-9]*'  as [ t ] | state == "parser_part": 
      [token_name, token_id] := split(t, ":");
      parser_token_list += [ Token(token_name, token_id) ];
    regex '\{:'  | state == "parser_part": 
      state := "codeseg";
    regex '.|\n' as [ u ] | state == "codeseg" : 
      user_code := user_code + u;            
    regex ':\}' | state == "codeseg" :
      uc := "UserCode('$user_code$')";
      parser_token_list += [ eval(uc)]; 
      user_code := "";
      state := "parser_part";
    regex '''[^'']*'''         as [ l ] | state == "parser_part":
     parser_token_list += [ Token(l, "") ];
      sc_dict[l] := "\\Q$l[2..-2]$\\E";
    regex '''[^'']*'':[a-z][a-zA-Z_0-9]*'         as [ l ] | state == "parser_part":
      [_, token_name, token_id] := matches(l, '(''[^'']*''):([a-z][a-zA-Z_0-9]*)', true);
      parser_token_list += [ Token(token_name, token_id) ];
      sc_dict[token_name] := "\\Q$token_name[2..-2]$\\E";
    regex '[ \t\v\n\r]+'               | state == "parser_part": 
        // skip
    regex '.|\n' as  [c]:
      if(!silent_mode){
        print("unrecognized character: $c$");
        print("current_state: $state$");
        print("line: ", map["line"]);
        print("column: ", map["column"]);
      }
  }
  if(sc_dict["SKIP"] != om){    
    old_skip   := sc_dict["SKIP"];
    new_skip   := "";
    dict_entry := "";
    scan(old_skip){
      regex '\{[A-Z]*\}' as [skip_token]:
        skip_token := skip_token[2..-2];
        if(sc_dict[skip_token] != om){
          dict_entry := sc_dict[skip_token];
          if(new_skip == ""){
            new_skip := "$dict_entry$";
          }
          else{
           new_skip  += "|$dict_entry$";
          }
          sc_dict[skip_token] := om;
        }
      regex '.|\n' as [c]:
        new_skip += c;
    }
    new_skip := replace(new_skip, ' \| ', '|');
    sc_dict["SKIP"] := new_skip;
  }   
  return [sc_dict, parser_token_list, comment];
};
generate_scan_string := procedure(sc_dict, silent_mode){
  scan_string   := "
  scan_string   := procedure(lines, silent_mode){
    scan_output := [];
    sc_dict     := $sc_dict$;\n";
  scan_string += "scan(lines){";
  for ( i in domain(sc_dict)){
    scan_string += "
    regex '$sc_dict[i]$' as c:
      if(#c > 1){
        //c := c[2..#c];
        c := c[1];
      }        
      c           := join(c, \"\");
      c           := join(split(c, '\"'), \"\"); 
      scan_output := scan_output + [[c, \"$i$\"]];";
  }
  scan_string += "
  regex '.' as [ c ]: if(!silent_mode){
    print(\"unrecognized character: \" + c);}";
  scan_string += "}";
  scan_string += "
  if(sc_dict[\"SKIP\"] != om){    
    scan_output := [scan_tokens : scan_tokens in scan_output | scan_tokens[2] != \"SKIP\"];
  }
  return scan_output;
  };";
  if(!silent_mode){
    print("scan_procedure:$scan_string$ \n");
  }
  return scan_string;
};

generate_parser_token_list := procedure(scanner_file, output_file_name, silent_mode){
  try{
    [sc_dict, parser_token_list, comment] := sc_dict_generate(scanner_file, silent_mode);
  }
  catch(e){
    print("error while generating scanner : $e$");
  }
  if(!silent_mode){
    dict_string := "tokens in scanner_dictionary: {";
    for(token in sc_dict){
      dict_string += "\n$token$,";
    }
    dict_string := dict_string[..-2];    
    dict_string += "\n}";
    print(dict_string);
  }
  scan_string := generate_scan_string(sc_dict, silent_mode);
  writeFile(output_file_name, "");
  if(comment != om){
    appendFile(output_file_name, comment);
  }
  appendFile(output_file_name, scan_string);
  appendFile(output_file_name, readFile("_sr_parser_part.stlx"));
  return parser_token_list;
};